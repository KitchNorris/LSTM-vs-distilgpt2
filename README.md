# Проект LSTM vs DistilGPT2 для предсказания текста

## Описание
Этот проект посвящён сравнению модели LSTM с предобученной моделью DistilGPT2 для задачи предсказания следующего слова в тексте. В проекте реализованы методы предобработки данных, обучение и оценка моделей, а также запуск предсказаний.

## Структура проекта
- `models/`  
  Папка с весами обученной LSTM-модели, предназначенной для предсказания слов.
  
- `requirements.txt`  
  Список необходимых библиотек Python для установки зависимостей проекта.

- `solution.ipynb`  
  Jupyter Notebook, демонстрирующий запуск всех скриптов из папки `src` и полный рабочий процесс.

- `src/`  
  Основные исходные коды проекта:
  - [`data_utils.py`](src/data_utils.py) — очистка и подготовка датасета.  
  - [`dist_gpt.py`](src/dist_gpt.py) — загрузка, запуск и оценка предобученной модели DistilGPT2 с помощью метрики ROUGE.  
  - [`lstm_model.py`](src/lstm_model.py) — класс модели LSTM.  
  - [`lstm_model_att.py`](src/lstm_model_att.py) — класс LSTM с механизмом внимания (attention).  
  - [`lstm_train_up.py`](src/lstm_train_up.py) — цикл обучения модели LSTM и вычисление метрик ROUGE.  
  - [`next_token_dataset.py`](src/next_token_dataset.py) — токенизация и разбиение данных на обучающую, валидационную и тестовую выборки.


